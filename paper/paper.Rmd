---
title: |
  New and simplified manual controls for projection and slice tours, with application to exploring classification boundaries in high dimensions
type: Short Technical Note
author:
  - name: Alex Aumann
    affil: a
    email: aaum0002@student.monash.edu
  - name: German Valencia
    affil: a
    email: german.valencia@monash.edu
  - name: Ursula Laa
    affil: b
    email: ursula.laa@boku.ac.at
  - name: Dianne Cook
    affil: c
    email: dicook@monash.edu
affiliation:
  - num: a
    address: |
      School of Physics and Astronomy, Monash University
  - num: b
    address: |
      Institute of Statistics, University of Natural Resources and Life Sciences, Vienna
  - num: b
    address: |
      Department of Econometrics and Business Statistics, Monash University
bibliography: biblio.bib
geometry: margin=2.5cm
abstract: |
  Something here
keywords: |
  data visualisation; grand tour; statistical computing; statistical graphics; multivariate data; dynamic graphics
header-includes: |
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \def\tightlist{}
  \usepackage{setspace}
  \usepackage{graphicx}
  \usepackage{nicematrix}
  \NiceMatrixOptions{code-for-first-row = \color{red} ,code-for-last-row = \color{red} ,code-for-first-col = \color{blue} ,code-for-last-col = \color{blue}}
output: rticles::tf_article
keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, 
  message = FALSE,
  warning = FALSE)
```

```{r libraries}
# Load libraries
library(tourr)
library(plotrix)
```

<!-- \doublespacing-->
# Introduction

<!-- Background -->
From a statistical perspective it is rare to have data that are strictly 3D, and so unlike in most computer graphics applications, the more useful methods for data analysis show projections from an arbitrary dimensional space. These are dynamic data visualizations methods and are collected under the term *tours*. Tours involve views of high-dimensional ($p$) data with low-dimensional ($d$) projections. In his original paper on the grand tour, @As85 provided several algorithms for tour paths that could theoretically show the viewer the data *from all sides*. Prior to Asimov's work, there were numerous preparatory developments including @tukey's PRIM-9. PRIM-9 had user-controlled rotations on coordinate axes, allowing one to manually tour through low-dimensional projections. (A video illustrating the capabilities is available through video library of @ASA22.) Steering through all possible projections is impossible, unlike Asimov's tours which allows one to quickly see many, many different projections. After Asimov there have been many, many tour developments, which are summarized in @lee2021. 

<!-- Manual tours -->
One such direction of work develops the ideas from PRIM-9, to provide manual control of a tour. @cook_manual_1997 describe controls for 1D (or 2D) projections, respectively in a 2D (or 3D) manipulation space, allowing the user to select any variable axis, and rotate it into, or out of, or around the projection through horizontal, vertical, oblique, radial or angular changes in value. @spyrison_spinifex_2020 refined this algorithm and implements them to generate animation sequences. 

<!-- Purpose -->
Manual controls are especially useful for assessing sensitivity of structure to particular elements of the projection. There are many places where it is useful. In exploratory data analysis, where one sees clusters in a projection, can some variables be removed from the projection without affecting the clustering. For interpreting models, one can reduce or increase a variable's contribution to examine the variable importance. These controls can also be used to interactively generate faceted plots [@XXX], or spatiotemporal glyphmaps [@XXX]. Having the user interact with a projection is extremely valuable for understanding high-dimensional data. However, these algorithms have two problems: (1) the pre-processing of creating a manipulation space overly complicates the algorithm, (2) extending to higher dimensional control is difficult. 

Through experiments with the relatively new interactive graphics capabilities in mathematica(?), we have realized that there is a simpler approach, which is more direct, and extensible for generating user interaction. This paper explains this, and is organized as follows. The next section describes the new algorithm for manual control. This is followed by details on implementation. The applications section illustrate how these can be used. 

# How to construct a manual tour {#sec:method}

A manual tour allows the user to alter the coefficients of one (or more) variables contributing to a $d-D$ projection. The initial ingredients are an orthonormal basis ($A_{p\times d}$) defining the projection of the data, and a variable id ($m \in \{1, ..., p\}$) specifying which coefficient will be changed. A method to update the values of the component of the controlled variable $V_m$ is then needed. 

## Existing methods

The method for updating component values in @cook_manual_1997 (and utilised in @spyrison_spinifex_2020) are prescribed primarily for a 2D projection, to take advantage of (then) newly developed 3D trackball controls made available for computer gaming. The first step was to construct a 3D manipulation space from a 2D projection. In this space the coefficient of the controlled variable ranges between -1 and 1. Movements of a cursor are recorded and converted into changes in the values of $V_m$ thus changing the displayed 2D projection. The algorithm also provided constraints to horizontal, vertical, radial or angular motions only. The construction of the manipulation space overly complicates the manual controls, especially when considering a technique that will generally apply to arbitrary $d$.

<!--
\begin{figure*}[ht]
\centerline{\includegraphics[width=0.8\textwidth]{figures/manip_space.pdf}}
\caption{Original contruction of the manual tour designed for 2D projections and created a 3D space from which to utilise track ball controls to change it's contribution. (Figure 3 from Cook and Buja (1997).)}
\label{manipspace}
\end{figure*}
-->

## A new simpler and broadly applicable approach

The new approach emerged from experiments in mathematica. The components corresponding to $V_m$ are directly controlled by cursor movement, which updates row $m$ of $A$. The updated matrix is then orthonormalised. 

### Algorithm 

1. Provide $A$, and $m$. (Note that $m$ could also be automatically chosen as the component that is closest to the cursor position.)
2. Change values in row $m$, giving $A^*$. A large change in these values would correspond to making a large jump from the current projection. Small changes would correspond to tracking a cursor, making small jumps from the current projection.
3. Orthonormalise $A^*$, using Gram-Schmidt. For $d=2$, and $A^* = \left[ {\boldmath a}_{.1}~{\boldmath a}_{.2}\right]$, the steps are:

    i. Normalise ${\boldmath a}_{.1}$, and ${\boldmath a}_{.2}$.
    ii. ${\boldmath a}^*_{.2} = {\boldmath a}_{.2} - {\boldmath a}_{.1}^T{\boldmath a}_{.2}{\boldmath a}_{.1}$.
    iii. Normalise ${\boldmath a}^*_{.2}$.

This algorithm will produce the changes to a projection as illustrated in Figure \ref{fig:manualsequence} (top row). The controlled variable, $V_m$, corresponds to the black line, and sequential changes to row $m$ of $A$ can be seen to roughly follow a specified position (orange dot). Changes in the other components happen as a result of the orthonormalisation, but are uncontrolled. 


```{r manualsequence, out.width="100%", width=12, height=4, fig.cap="Sequence of projections where contribution of one variable is controlled (black) is changed using unconstrained orthonormalisation. The dot (orange) indicates the chosen values for the controlled variable. It can be seen that the actual axis does not precisely match the chosen position, but it is close."}
source("../code/plot_basis.R")
source("../code/linear_alg.R")

# Base plot set up
plot.new()
par(pty="s", xaxt="n", yaxt="n", bty="n",
      omi=c(0,0,0,0), mar=c(0,0,0,0),
    mfrow=c(2,4))

# Create manual tour example
p <- 4
d <- 2
set.seed(24)
A <- matrix(runif(p*d, min=-1), ncol=d, byrow=TRUE)
colnames(A) <- c("P1", "P2")
rownames(A) <- paste0("V", 1:p)
A <- tourr::orthonormalise(A)
  
# Now set a new position
vchange <- 3
eps <- 0.1
plot_basis(A, vchange, vcol="black")

# Iterate and save images
# Unconstrained orthonormalisation
Anew <- A
for (i in 1:3) {
  Anew[vchange,] <- A[vchange,]*(1-i*eps)
  exact <- Anew[vchange,]
  Anew <- tourr::orthonormalise(Anew)
  plot_basis(Anew, vchange, vcol="black")
  points(exact[1], exact[2], pch=16, col="orange")
}

# Iterate and save images
# Constrained orthonormalisation
#Anew <- A
#plot_basis(A, vchange, vcol="black")
#for (i in 1:3) {
#  Anew[vchange,] <- A[vchange,]*(1-i*eps)
#  exact <- Anew[vchange,]
#  Anew <- orthonormalise_frozen(Anew, vchange)
#  plot_basis(Anew, vchange, vcol="black")
#  points(exact[1], exact[2], pch=16, col="orange")
#}
```

## Refinements to enforce exact position

The problem with new simple method (Algorithm 1) is that the precise values for $V_m$ are not followed because the orthonormalisation will change them. There are numerous ways that this can be enforced, and details are provided in the Appendix.


## Manual control for slice center

How does it work?

Diagram of interface? Scaling

# Experimenting with new techniques using Mathematica {#sec:implementation}

- **Why is this a good sandbox**
- **Explain the functionality available in the notebooks**

<!-- 
The following functions are implemented in the Mathematica package `mmtour.wl`:

- `ProjectionPlot[data, projmat, xRange(=Automatic), yRange(=Automatic), 
colour(=Automatic)]` with arguments

    - `data`: A list of data matrices (they must all have the same dimensionality)
    - ``projmat`: A projection matrix which describes the projection plane. This is the 
initial projection and does not need to be orthonormal (XXX is this right?).
    - ``xRange`: A list which details the range of the horizontal axis.
    - ``yRange`: A list which details the range of the vertical axis.
    - ``colour`: A list of graphics directives to be applied to each data matrix.

There are problems with the default values for x and y ranges and colour. It would be 
better to fix the ranges and colour based on the data as well as the origin of axes 
and the size of the points.

**It would be good if the initial projection matrix (and maybe the center) had default values. Perhaps a random matrix with the correct dimensions read from the data? Fix 1**

The format for the printed projection matrix can be improved also. Maybe fix the 
number of digits and the position of the label.

- `SlicePlot[data, projmat, centrePoint, height, heightRange, ptSize1(=0.005), ptSize2(=0.004), minDist(=0)]` with arguments (used to be called SliceDynamic in Alex's first version, the name SliceDynamic was taken by something else below)

    - `data`: A data matrix
    - `projmat`: The inital projection matrix
    - `centrepoint`: The initial position of the slice
    - `height`: The initial height of the slice
    - `heightRange`: The range of heights
    - `ptSize1`: The size of the points that exist within the slice
    - `ptSize2`: The size of the points that exist outside of the slice

This function requires some auxiliary functions. It does not accept more than one data set.

**It would be good if the initial projection matrix (and maybe the center) had default values. Perhaps a random matrix with the correct dimensions read from the data?**

It may be good to fix the size of the points, the center of the axes and the scales of the axes rather than allow them to change as the projection and slicing changes

- `SliceDynamic[data, projmat, centrePoint, height, heightRange, ptSize1 : 0.005, ptSize2 : 0.004]` with arguments

- `data`: A data matrix with an extra grouping or clustering column (numerical) 
- `projmat`: The initial projection matrix
- `centrepoint`: The initial position of the slice
- `height`: The initial height of the slice
- `heightRange`: The range of heights
- `ptSize1`: The size of the points that exist within the slice
- `ptSize2`: The size of the points that exist outside of the slice

**Does this need to be a separate function from SlicePlot? Fix 2**

**It would be good if the initial projection matrix (and maybe the center) had default values. Perhaps a random matrix with the correct dimensions read from the data? Fix 3**

**It may be good to fix the size of the points, the center of the axes and the scales of the axes rather than allow them to change as the projection and slicing changes. Fix 4**

**I have found it useful to input different data sets rather than have this function sort them automatically using the last column. One data set with a flag column at the end, is best! Is the code too complicated? Fix 5**

- `Projected2DSliderPlot[]`

This function uses a different display for the projection that I find very useful. However as it stands it is specific to one data set. Needs to be generalised to arbitrary data if we want to keep it.

**For Alex to do**

- **We would need a screenshot (and video) for each of these.**
- **Fix 1, 2, 3, 4, 5**
- **Code consistent for applications to work - see slice_tour_compare_sets.nb. See below**
    - **We need to make sure the drawing range is fixed (no resizing of the axis when changing projection or slicing). It could be nice if there was a manual zoom in/zoom out to change scale occasionally.**
    - **Should also make sure the size of the points is fixed (maybe this will be automatic if the drawing range is fixed?), bigger points would be preferred, currently the size is sometimes really tiny and it becomes difficult to see anything.**
    - **Can we have some heuristic to decide the range for the slice thickness slider? In the second graph there is a tiny part of the slider where the changes are interesting, difficult to navigate since this makes it super sensitive to any small movement, while most of the options are not interesting. My suggestion would be to keep (as I understand is the case now) the maximum thickness (all points in) as the upper bound, but instead of zero use a thickness below which slices tend to be empty as the lower bound.**
    - **For the olives example I think the relation between x2 and the blue region could be interesting.**
    - **pdfsense data: can we use random samples instead of the first 10k rows? Then we should still be able to see the bigger picture (with picking the first ones we get some artificial structure from how the data was sampled initially).**
    - **I was playing around with this today. Looks like the axes are fixed when changing slice thickness, but not when rotating?**
    - **Anyways, some observations that might be useful, but all of this seems difficult for including in a paper:**
    - **In the first example we can show how the yellow group is associated with the first variable (x1, I guess this is C9?) - when slicing through the center it only appears when x1 has a bigger component on the projection, it is associated with low values of x1. Since they are not near the average value of x1 the points do not get captured in a slice where x1 is not important in the projection. We can move the center point to -1 in the first component to then explore how the yellow group relates to the other three variables. Much less clear, maybe some indication of smaller x2 and larger x3 for this.** 
    - **The second example is too busy for manual exploration I would say (too many axes to move around separately).**
    - **Third example could be similar to the first: blue region requires small values of x2, can be understood from the exploration, then move center point to -0.5 in x2 to explore further. Now interpretation becomes tricky. Sometimes blue and yellow seem just randomly everywhere, sometimes blue more contained in an ellipsoid around the center (indicating some correlation between variables). For example try zeroing out components 2 and 3 to see this slice in x1 vs x4.**
    - **For the last example x4 seems to be the one to work with. I feel like this is again very similar, maybe better to instead explore other examples (maybe something without slicing?)**
-->

```{r penguins-scatmat, out.width="80%", width=10, height=10, fig.align='center', fig.cap="Scatterplot matrix of the (standardised) penguins data. The three species are reasonably different in size, with Gentoo distinguised from the other two on body depth relative to flipper length and body mass."}
library(tidyverse)
library(palmerpenguins)
library(GGally)
penguins_data <- penguins[, c(1,3, 4, 5, 6)] %>%
  drop_na() %>%
  mutate(across(where(is.numeric), function(x) (x-mean(x))/sd(x))) %>%
  rename("bl"="bill_length_mm",
         "bd"="bill_depth_mm",
         "fl"="flipper_length_mm",
         "bm"="body_mass_g")
ggscatmat(penguins_data, columns = 2:5, col="species") +
  scale_colour_manual("", values=c("Adelie" = "#5E82B6", "Chinstrap" = "#E19C24", "Gentoo" = "#8FB132")) +
  theme(legend.position = "bottom")
```


# Application {#sec:examples}

To illustrate the usefulness of the manual controls we use the 4D penguins data [@penguins]. We will show how classification boundaries can be explored and better understood on projections and slices through 4D space. Figure \ref{fig:penguins-scatmat} shows a scatterplot matrix of this data. There are four variables (`bl = bill_length_mm, bd = bill_depth_mm, fl = flipper_length_mm, bm = body_mass_g`) measuring the size of the penguins from three species (Adelie, Chinstrap and Gentoo). The scatterplot matrix shows that the three species appear to be likely separable, and that at least the Gentoo can be distinguished from the other two species when `bd` is paired with `fl` or `bm`. The steps for exploring boundaries in this example are as follows:

1. Build your classification model. 
2. Predict the class for a dense grid of values covering the data space.
3. Examine projections, using a manual tour so that the contribution of any variable is controlled.
4. Slice through the center, to explore where the boundaries will likely meet.
5. Move the slice by changing the center in the direction of a single variable to explore the extent of a boundary for a single group relative to a variable.


## Constructing the 4D prediction regions

We use the `classifly` package [@classifly] to generate predictions across the 4D cube spanned by the data, with two classification models: linear discriminant analysis (LDA) and random forest (RF).

## Exploring projections manually

We start by first exploring the projections of the model prediction. Figure \ref{proj1} summarises the process. Through manual rotation of the view we can get a feeling for where in the space we primarily predict each of the three species, and we also get a sense of the difference between the two models. To illustrate this difference we have manually rotated the projection for the RF model (left plot) to identify a projection that shows the non-linear but block-type structure that is typical of this type of model. This particular projection ($A_1$) is exported so that the same projection can be used to show the LDA model (middle plot) and the actual data (right plot). What can be seen is the linearity of the LDA model, where the boundaries are linear and oblique to the variable axes. And, interestingly this particular projection of the original data shows very distinct clusters of the three species. That means, the obscuring of the boundaries between groups for both of the models is driven by what is happening in the orthogonal space to the plane of the selected projection.

\begin{figure*}[ht]
\centerline{\includegraphics[width=0.32\textwidth]{figures/proj1_rf.png}
\includegraphics[width=0.32\textwidth]{figures/proj1_lda.png}
\includegraphics[width=0.32\textwidth]{figures/proj1_data.png}}
\caption{Projection identified using the manual tour, because it reveals an interesting structure in the predictions from the RF model (left). We can clearly see a block structure, while the LDA model (middle) produces linear boundaries. The three groups are nicely separated in this projection of the data (right)}.
\label{proj1}
\end{figure*}

## Slicing through the center

XXXX currently in the notebook $A_1$ is called A5, and $A_2$ is called A6

We continue the investigation by now slicing orthogonally to  the projection $A_1$. For both models we look at a thin ($h=0.5$) slice through the center, $S_1^0 = (0,0,0,0)$. At  first we explore how changing the projection, and thus the slice) away from $A_1$ can help with understanding the boundary better. For our example, notice that $A_1$ does not contain any contribution from the second variable (`bd`), so we will first rotate this variable into the view. 

Figure \ref{slice1} shows snapshots of the exploration. The rop row is the initial projection ($A_1$ and $S_1^0$), and the bottom row is a later projection containing more of `bd` (call them $A_2$ and $S_2^0$). The columns correspond to the two models, with Rf on the left and LDA on the right.

Both models have some overlap at the center, even for this thin slice ($S_1^0$). The second slice ($S_2^0$) mostly resolves this overlap and reveals the primary differences between the models. The boundary between Adelie (blue) and Chinstrap (yellow) is very similar for both models. The boundary between Chinstrap and Gentoo (green) is where they differ. 

The RF is almost straight in this view, as something we might expect from a tree model where splitting occurs on single variables only. However, it is straight in a combination of the second and third variable (`bd` and `fl`). By examining the scatterplot matrix, we can understand how this boundary was built: on each of `bd` and `fl` it is possible to cut on a value where most of the Gentoo penguins are different from the other two speciesfor any tree, and likely only one is needed. Thus the forest construction is providing a sample of trees that use one variable or the other variable to split, producing the blocky boundary on the combination of the two. 

In contrast, the LDA boundary has an oblique split between the two, and roughly divides the space into three similarly sized areas. It is almost like a textbook illustration of how LDA works for two variables (2D) where by assuming the clusters have equal variance-covariance, it places a boundary half-way between the group means. 

<!-- We again start by looking at the RF model, the slice $S_1^0$ shows the block structure of the model, with the third group (green, Gentoo penguins) overlapping with both of the other groups (Fig. \ref{slice1}, top-left). This is similar for the LDA model (Fig. \ref{slice1}, top-right), but again the linearity results in different boundaries and thus differences in how the classes overlap.

By rotating in the second variable we can find a view that shows three neatly separated blocks for the RF model, and we export the corresponding projection matrix $A_2$ to see the same views for the LDA model which also shows a neater separation of the classes along the boundary (see bottom row of Fig. \ref{slice1}).-->

Finally, to determine which model does the boundaries better, compare them with the $A_2$ projection of the data (Figure \ref{proj2}). Gentoo is more separated from the other two in this projection, and one can imagine that the trees in the forest has greedily grasped any one of many places to make a split to separate the group. It might be argued though the that RF boundary is cut too close to the Chinstrap species, and might lead to some unnecessary misclassification with new data. The LDA boundary is better placed for all species. 

\begin{figure*}[ht]
\centerline{\includegraphics[width=0.45\textwidth]{figures/slice1_rf.png}
\includegraphics[width=0.45\textwidth]{figures/slice1_lda.png}}
\centerline{\includegraphics[width=0.45\textwidth]{figures/slice2_rf.png}
\includegraphics[width=0.45\textwidth]{figures/slice2_lda.png}}
\caption{Comparing slices based on two projections $A_1$ (top row) and $A_2$ (bottom row), for the two models RF (left) and LDA (right). With $A_1$ we see two groups overlap (green - Gentoo with yellow - Chinstrap), while the rotation to $A_2$ results in clear boundaries inside the slice. The boundary between Adelie (blue) and Chinstrap (yellow) is similar for both models but very different between Chinstrap and Gentoo (green).}
\label{slice1}
\end{figure*}

\begin{figure*}[ht]
\centerline{\includegraphics[width=0.45\textwidth]{figures/proj2_data.png}}
\caption{Projection of the data based on $A_2$. Compared to projecting onto $A_1$ we see that the green observations (Gentoo) are more separated from the other two species.}
\label{proj2}
\end{figure*}


## Shifting the slice center

We have seen that starting from $A_1$ using the manual controls to change the contribution of the second variable we could find a clear separation boundary indicating the relation between this variable (bill depth) and the Gentoo penguin species. Instead of rotating to a different projection, we might also change the view by moving the slice along one direction in the 4D space. Here we will continue our exploration of the dependence on `bd` and move the slice defined by $A_1$ to either large or small values of the bill depth ($\pm 1.5$ after centering and scaling). We will label these slices as $S_1^{\pm}$. Here we will also look at slices of the observed data points, using a thicker slice ($h=1.5$) to capture enough points in a given view.

We start by a comparison of the two models and the data distribution in $S_1^{+}$, thus the slice is localized towards high values of bill depth in Fig. \ref{slice1p}. We can see that all three slices (the two models and the data) contain almost no points from the third class (green, Gentoo), and that the decision boundary between the two models is very similar.

\begin{figure*}[ht]
\centerline{\includegraphics[width=0.32\textwidth]{figures/slice1_p_rf.png}
\includegraphics[width=0.32\textwidth]{figures/slice1_p_lda.png}
\includegraphics[width=0.32\textwidth]{figures/slice1_p_data.png}}
\caption{...}.
\label{slice1p}
\end{figure*}


A more interesting comparison is found for $S_1^{-}$, thus the slice localized towards low values of bill depth, shown in Fig. \ref{slice1m}. The RF model (left) predicts all three species within this slice, with an interesting boundary for the third class (green, Gentoo). On the other hand the LDA model (middle) predominantly predicts the third class within the slice, this appears to be enforced through the linear structure of the model. Looking finally at the thick slice through the data we see that there are primarily observations from this class within the slice we can conclude that the two models have filled in the "empty" space (where we do not have any training observations) in very different ways and according to what we might expect given the model structure.

\begin{figure*}[ht]
\centerline{\includegraphics[width=0.32\textwidth]{figures/slice1_m_rf.png}
\includegraphics[width=0.32\textwidth]{figures/slice1_m_lda.png}
\includegraphics[width=0.32\textwidth]{figures/slice1_m_data.png}}
\caption{...}.
\label{slice1m}
\end{figure*}

Finally it is interesting to compare the slice views to the projection of the models seen in Fig. \ref{proj1} to better understand how the boundaries change along the `bd` direction and where the differences in the projections come from.

# Extension added in the R package `tourr`

Explain `radial_tour`

# What would be desirable for implementations in R?

# Discussion {#sec:discussion}


# Acknowledgements {-}

The authors gratefully acknowledge the support of the Australian Research Council. The paper was written in `rmarkdown` [@rmarkdown] using `knitr` [@knitr]. 

# Supplementary material {-}

The source material and animated gifs for this paper are available at 
